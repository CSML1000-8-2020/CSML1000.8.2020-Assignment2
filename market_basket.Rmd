---
title: "CSML1000 Winter 2020, Group 8, Assignment2: Market Basket Prediction Model"
author: "Steven Wang, Tarun Bagga, Paul Doucet, Jerry Khidaroo, Nikola Stevanovic"
date: "2/15/2020"
# output:
#   html_document:
#   toc: TRUE
#   toc_depth: 2
output:
  pdf_document:
    toc: TRUE
    toc_depth: 3
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{css, echo=FALSE}
.ethical-text {
  background-color: lightblue;
  border: 3px solid blue;
  font-weight: italic;
}
```

## Load Libraries

```{r, message = FALSE}
# Load packages
# library('ggplot2') # visualization
# library('ggthemes') # visualization
# # library('scales') # visualization
# library('dplyr') # data manipulation
# library('caret')
# library('caret')
# library('readr')
# library('data.table')
# library('knitr')
# library('purrr') # ANN modeling functionality
# library('arules')

library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(lubridate)
library(arules)
library(arulesViz)
library(plyr)
```

## 1. Business Understanding

- Business Problem: The Business problem we want to solve is how to increase the number of items ordered by a customer in each order submitted in the online store. To acheive this goal we want to build a machine learning model that predicts the most likely items a customer will add to their basket next based on their current selection and display those items as suggestions for quick access.

- Project Plan: 
  - Load and get an understanding of the dataset, its target variable and its features.
  - Make any modifications to the dataset needed to enable learning algorithms to be run on the data.
  - Identify the features of the dataset that are important in predicting the output recomendations.
  - Build and evaluate 2 Models by appling unsupervised machine learning algoritms to the dataset  as appropiate and testing them.
  - Identify the best model to use for the project.
  - Build a shiny app that deploys the selected model with a user interface that allows end users to add products to a basket and get a list of recomended products.
  - Identify any ethical considerations that should be addressed at each stage of the process.

- Business Success Criteria: 

- Ethical Framework Questions: 
  - How could your system negatively impact individuals? The greatest negative impact would occur for a false positive diagnosis since this could delay treatment and in a life threatening scenario. A false negative would also be negatively impactful but to a lessor degree.
  - Who is most vulnerable and why? The most vulnerable would be patients with a 'M' dianosis not detectable by the model.
  - How much error in predictions can your business accept for this use case? False positives need to be minimised as much as posible. Minimsing False negatives are second in priority.
  - Will you need to explain which input factors had the greatest influence on outputs? Yes. Being able to explain which features have the most influence on outcome is very desirable.
  - Do you need PII or can you provide group-level data? The analysis requires patient level data however any PII can be anonymised

## 2. Data Understanding

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>

<div class = "blue">

- Ethical Framework Questions: 
  - Have you de-identified your data and taken measures to reduce the probability of reidentification? The data is de-identified.
  - Will socially sensitive features like gender or ethnic background influence outputs? No demographic data is present.
  - Are seemingly harmless features like location hiding proxies for socially sensitive features? No demographic data is present. 
</div>

#### 2.1 Get Data Files

- The Dataset used is obtained from: https://www.kaggle.com/c/instacart-market-basket-analysis

#### 2.2 Initial Data Collection Report:

  - There are seven files provided as part of the dataset for this unsupervised learning example:
    1. aisles.csv: 
    2. departmenst.csv:
    3. order_products__prior.csv:
    4. order_products__traing.csv:
    5. orders.csv
    6. products.csv
    7. sample_submission.csv (This file will not be used in the analysis since it is only relevant for submission to the kaggle competition where the data was sourced.)

#### 2.3 Load and check data

```{r, message = FALSE}
orders = read.csv("./input/orders.csv")
aisles = read.csv("./input/aisles.csv")
departments = read.csv("./input/departments.csv")
products = read.csv("./input/products.csv")
order_products_prior = read.csv("./input/order_products__prior.csv")
order_products_train = read.csv("./input/order_products__train.csv")
```

```{r, message = FALSE}
# check data
kable(head(aisles))
kable(departments)
kable(head(products))
kable(head(orders))
kable(head(order_products_prior))
kable(head(order_products_train))
```

## 3. Data Preparation

###### We also know from the data file descriptions from the source website that order_products_train has the most recent procuct orders by the user and order_products_prior contains all of the historical product orders.

#### 3.1 Data Modification

###### Due to data storage and processing constraints this analysis will use a random sample of the dataset available rather than all records.

###### Get a sample of 10000 'most recent' orders

```{r, message = FALSE}
# Set seed for reproducibility
set.seed(1234)
orders_train <- subset(orders, eval_set=='train')
orders_train_sample = orders_train[sample(nrow(orders_train), 10000), ]
#write.csv(orders_sample, "./input/orders_train_sample.csv", row.names = F)
```

###### merge order_products_prior with products and orders tables

```{r, message = FALSE}
# Merge products columns into order_products_prior
tmp <- merge(order_products_prior, products, by.x="product_id", by.y="product_id")

# Merge Orders columns into order_products_prior
orders_prior <- subset(orders, eval_set=='prior')
orders_products_prior_merged <- merge(tmp, orders_prior, by.x="order_id", by.y="order_id")
```

###### Filter orders_products_prior_merged based on user_ids in orders_train_sample

```{r, message = FALSE}
order_products_prior_sample = subset(orders_products_prior_merged, user_id %in% orders_train_sample$user_id)
```

###### merge order_products_train with products and orders tables

```{r, message = FALSE}
# Merge products columns into order_products_train
tmp <- merge(order_products_train, products, by.x="product_id", by.y="product_id")

# Merge Orders columns into order_products_train
orders_products_train_merged <- merge(tmp, orders_train, by.x="order_id", by.y="order_id")
```

###### Filter order_product_train based on user_ids in orders_sample

```{r, message = FALSE}
order_products_train_sample = subset(orders_products_train_merged, user_id %in% orders_train_sample$user_id)
```

###### Since we will be performing an unsupervised machine learning analysis on the dataset, records from order_products_prior_sample and order_products_train_sample can be combined.

###### Join order_products_prior_sample and order_products_train_sample into a single dataset


```{r, message = FALSE}
order_products_sample_combined <- rbind(order_products_prior_sample, order_products_train_sample)

# Lets look at the merged sample dataset we will proceed with in our analysis
kable(head(order_products_sample_combined, 12))
```
#### 3.2 Feature Engineering

###### - For the Market Basket analysis we will need 3 data columns in order to build a transaction file:

###### - A Customer identity field, an Transaction field, and a Product Name field

###### - From looking at the features we have orders$user_id for Customer identity, orders$order_id for Transaction, and products$product_name for Product Name.

```{r, message = FALSE}
# Select the 3 columns we need to create the transactions dataset and inspect
mydata <- subset(order_products_sample_combined, select = c(user_id, order_id, product_name))
kable(head(mydata, 12))
```

###### These 2 files will now be saved to disk for future re-loading during project development

```{r, message = FALSE}
# Save files
write.csv(order_products_sample_combined, "./input/order_products_sample_combined.csv", row.names = F)
write.csv(mydata, "./input/mydata.csv", row.names = F)
```

###### Create Transaction Table 

```{r, message = FALSE}
# Create Transaction Table
detach("package:plyr", unload=TRUE)

retail_sorted <- mydata[complete.cases(mydata),]
retail_sorted <- retail_sorted[order(retail_sorted$user_id),]
library(plyr)
itemList <- ddply(retail_sorted,c("user_id","order_id"),
                  function(df1)paste(df1$product_name,
                                     collapse = ","))

# Remove user and order id's from the list
itemList$user_id <- NULL
itemList$order_id <- NULL
colnames(itemList) <- c("Items")
```

```{r, message = FALSE}
# Save Transaction File
write.csv(itemList,"./input/InstaCart_MBA.csv", quote = FALSE, row.names = TRUE)
```

- Clear up some memory

```{r, message = FALSE}
rm(orders)
rm(order_products_prior)
rm(order_products_train)
rm(orders_products_train_merged)
rm(orders_train)
rm(orders_train_sample)
rm(order_products_prior_sample)
rm(order_products_train_sample)
rm(orders_products_prior_merged)
rm(orders_prior)
rm(order_products_sample_combined)
rm(mydata)
gc()
```
###### Once the analysis data files are saved the data preparation steps above can be commented out and we can just:

###### Load the saved files

```{r, message = FALSE}
order_products_sample_combined <- read.csv("./input/order_products_sample_combined.csv")
mydata <- read.csv("./input/mydata.csv")
```

## 4. Data Understanding (Continued)

###### Back to some more detailed examination of the data now that we have our analysis dataset in place

###### Take a look at data distribution by Dept. id. 

```{r, message = FALSE, fig.width=8, fig.align='left'}
a=table(order_products_sample_combined$department_id)
barplot(a,main="Using BarPlot to display Dept.Id Comparision",
        ylab="Count",
        xlab="department_id",
        col=rainbow(5),
        #legend=rownames(a)
        )
```

###### Take a look at data distribution by Day of Week. 

```{r, message = FALSE, fig.width=8, fig.align='left'}
density1 <- seq(5,35,length.out=7)
a=table(order_products_sample_combined$order_dow)
wth <- c(rep(5, each=7))
barplot(a,main="Using BarPlot to display Day of Week Comparision",
        ylab="Count",
        xlab="department_id",
        col=rainbow(2, start = 0.5, end = 0.7),
        density=density1,
        #legend=rownames(a),
        width = wth)
```

###### Take a look at data distribution by Hour of Day. 

```{r, message = FALSE, fig.width=8, fig.height=5, fig.align='left'}
a=table(order_products_sample_combined$order_hour_of_day)
barplot(a,main="Using BarPlot to display Hour of Day Comparision",
        ylab="Count",
        xlab="department_id",
        col=rainbow(22, start = 0.5, end = 0.7),
        #legend=rownames(a),
        #width = rep(2, each=24)
        )
```

```{r, message = FALSE}
# tmp <- retail %>%
#   group_by(product_id, product_name) %>%
#   summarize(count = n()) %>%
#   arrange(desc(count))
# tmp <- head(tmp, n=10)
# tmp
# tmp %>%
#   ggplot(aes(x=reorder(product_name,count), y=count))+
#   geom_bar(stat="identity",fill="indian red")+
#   coord_flip()
```

## 5. Data Modeling

- Ethical Framework Questions: 
  - Does your use case require a more interpretable algorithm? Accuracy is more impotant than interpretability for this study.
  - Should you be optimizing for a different outcome than accuracy to make your outcomes fairer? Since there is no demographic data, fairness would be hard to determine.
  - Is it possible that a malicious actor has compromised training data and created misleading results? No. The data is from a reputable source.

#### 5.1 Data Modeling - Apriori

```{r, message = FALSE}
# Load Transaction data
suppressWarnings(
tr <- read.transactions('./input/InstaCart_MBA.csv', format = 'basket', sep=',')
)
```

###### 5.1.1 Build Model - Apriori

```{r, message = FALSE}
tr
summary(tr)

itemFrequencyPlot(tr, topN=20, type='absolute', col=rainbow(20, start = 0, end = 0.6))
# Training Apriori on the dataset
rules <- apriori(tr, parameter = list(supp=0.001, conf=0.5))
rules <- sort(rules, by='confidence', decreasing = TRUE)
summary(rules)
inspect(rules[1:20])
```

#### 5.2 Data Evaluation - Apriori

###### 5.2.1 Test the Model

```{r, message = FALSE, fig.align='left'}
topRules <- rules[1:20]
# Visualising the results
plot(topRules, method="graph", shading=NA)
```

```{r, message = FALSE, fig.height=8}
plot(topRules, method = "grouped")
```

```{r, message = FALSE}
plot(topRules, method = "two-key plot")
plot(topRules, method = "scatterplot")
plot(topRules, method = "paracoord")
# plot(topRules, measure=c("support","lift"),shading="confidence",interactive=T)
#interestMeasure(rules.pruned, c("support", "chiSquare", "confidence", "conviction", "cosine", "coverage", "leverage", "lift", "oddsRatio"), titanic.raw)
```


```{r, message = FALSE}

```

###### 5.2.2 Perform a test prediction

```{r, message = FALSE}
grocery_item = "Milk"
rules <- apriori(tr, parameter = list(supp=0.001, conf=0.15), 
                 appearance = list(default="rhs", lhs=grocery_item), 
                 control = list (verbose=F))
rules_conf <- sort (rules, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
result = inspect(head(rules_conf))
result$rhs
```

#### 5.3 Data Modeling - Eclat

```{r, message = FALSE}
# Training Eclat on the dataset
eclat_itemsets = eclat(tr, parameter = list(support = 0.001, minlen = 2))
eclat_itemsets <- sort(eclat_itemsets, by='count', decreasing = TRUE)
summary(eclat_itemsets)

inspect(sort(eclat_itemsets, by = 'count')[1:10])
eclat_topItemSets <- eclat_itemsets[1:10]
```

#### 5.4 Data Evaluation - Eclat

```{r, message = FALSE, fig.align='left'}
# Visualising the results of the Eclat Analysis
plot(eclat_topItemSets, method="graph")
plot(eclat_topItemSets, method = "scatterplot")
plot(eclat_topItemSets, method = "paracoord")
```

###### 5.4.1 Perform a test prediction

```{r, message = FALSE}
grocery_item = "Garlic"
# eclat_itemsets = eclat(tr, parameter = list(support = 0.001, minlen = 2), 
#                  control = list (verbose=F))

## Create rules from the itemsets
eclat_rules <- ruleInduction(eclat_itemsets, tr, confidence = .9)
summary(eclat_rules)
kable(inspect(eclat_rules[1:20]))

result_eclat <- subset(eclat_rules, subset = lhs %in% "Milk")

rules_conf <- sort (result_eclat, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
result = inspect(head(rules_conf))
result$rhs
```

## 6. Final Model Analysis and Selection

#### 6.1 Cost Analysis

#### 6.2 Model Comparison

- The following Machine Learning Algorithms were used in this analysis:
  - Apriori
  - Eclat

```{r, message = FALSE}

```

#### 6.3 Selected Model: 

## 7. Deployment

#### 7.1 Shiny App Url: 

#### 7.2 Summary Explanation

- Limitations of our analysis: 
  - Due to processing and resource limitations we used a random sample of aproximately 10% of the original dataset
  - The analysis is based on data provided by InstaCart and may inherit any biases that exists in their customer base relative to the general population.
  
- Further steps: 
  - The analysis can be expanded to include all of the original data as well as any other similar sources that may be available.
  - 
  
- Explanation of Model:
  - The model 
  - The following Factors contributed to the success of the model in predicting user ordering behaviour: 

- Ethical Framework Questions: 
  - Can a malicious actor infer information about individuals from your system? No. There is no PII present.
  - Are you able to identify anomalous activity on your system that might indicate a security breach? This would need to be considered for each specific deployment.
  - Do you have a plan to monitor for poor performance on individuals or subgroups? N/A since No demographic data is present.
  - Do you have a plan to log and store historical predictions if a consumer requests access in the future? N/A since No demographic data is present.
  - Have you documented model retraining cycles and can you confirm that a subjectâ€™s data has been removed from models? N/A since No demographic data is present.

## References

Yihui Xie, J. J. Allaire, Garrett Grolemund, 2019, R Markdown: The Definitive Guide
https://bookdown.org/yihui/rmarkdown/markdown-syntax.html

Jonathan McPherson, 2016, R Notebooks
https://blog.rstudio.com/2016/10/05/r-notebooks

Adam Kardash, Patricia Kosseim, 2018, Responsible AI in Consumer Enterprise, integrate.ai


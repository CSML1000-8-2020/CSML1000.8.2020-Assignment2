---
title: "CSML1000 Winter 2020, Group 8, Assignment2: Market Basket Prediction Model"
author: "Steven Wang, Tarun Bagga, Paul Doucet, Jerry Khidaroo, Nicola Stevanovic"
date: "2/15/2020"
# output:
#   html_document:
#   toc: TRUE
#   toc_depth: 2
output:
  pdf_document:
    toc: TRUE
    toc_depth: 2
    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- ```{r} -->
<!-- par(mar=c(5,5,0,0)) #it's important to have that in a separate chunk -->
<!-- ```  -->

## Load Libraries

```{r, message = FALSE}
# Load packages
# library('ggplot2') # visualization
# library('ggthemes') # visualization
# # library('scales') # visualization
# library('dplyr') # data manipulation
# library('caret')
# library('caret')
# library('readr')
# library('data.table')
# library('knitr')
# library('purrr') # ANN modeling functionality
# library('arules')

library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(lubridate)
library(arules)
library(arulesViz)
library(plyr)
```

## 1. Business Understanding

- Business Problem: The Business problem we want to solve is how to increase the number of items ordered by a customer in each order submitted in the online store. To acheive this goal we want to build a machine learning model that predicts the most likely items a customer will add to their basket next based on their current selection and display those items as suggestions for quick access.

- Project Plan: 
  - Load and get an understanding of the dataset, its target variable and its features.
  - Make any modifications to the dataset needed to enable learning algorithms to be run on the data.
  - Identify the features of the dataset that are important in predicting the target variable.
  - Build and evaluate 3 to 4 Models from the dataset by appling various machine learning algoritms as appropiate and testing them.
  - Identify the best model to use for the project.
  - Build a shiny app that deploys the selected model with a user interface for end users to imput measurement values from a study and obtain a pridiction result.
  - Identify any ethical considerations that should be addressed at each stage of the process.

- Business Success Criteria: 

- Ethical Framework Questions: 
  - How could your system negatively impact individuals? The greatest negative impact would occur for a false positive diagnosis since this could delay treatment and in a life threatening scenario. A false negative would also be negatively impactful but to a lessor degree.
  - Who is most vulnerable and why? The most vulnerable would be patients with a 'M' dianosis not detectable by the model.
  - How much error in predictions can your business accept for this use case? False positives need to be minimised as much as posible. Minimsing False negatives are second in priority.
  - Will you need to explain which input factors had the greatest influence on outputs? Yes. Being able to explain which features have the most influence on outcome is very desirable.
  - Do you need PII or can you provide group-level data? The analysis requires patient level data however any PII can be anonymised

## 2. Data Understanding

- Ethical Framework Questions: 
  - Have you de-identified your data and taken measures to reduce the probability of reidentification? The data is de-identified.
  - Will socially sensitive features like gender or ethnic background influence outputs? No demographic data is present.
  - Are seemingly harmless features like location hiding proxies for socially sensitive features? No demographic data is present. 

#### Get Data Files

- The Dataset used is obtained from: https://www.kaggle.com/c/instacart-market-basket-analysis

#### Load and check data

#### Run once to create order_products_sample_combined.csv

<!-- ```{r, message = FALSE} -->
<!-- orders = read.csv("./input/orders.csv") -->
<!-- orders_train <- subset(orders, eval_set=='train') -->
<!-- orders_prior <- subset(orders, eval_set=='prior') -->
<!-- ``` -->

<!-- #### Load data files -->

<!-- ```{r, message = FALSE} -->
<!-- # full_file_name <- paste("./input/market_basket_data.csv",sep="") -->
<!-- aisles = read.csv("./input/aisles.csv") -->
<!-- departments = read.csv("./input/departments.csv") -->
<!-- products = read.csv("./input/products.csv") -->
<!-- order_products_prior = read.csv("./input/order_products__prior.csv") -->
<!-- order_products_train = read.csv("./input/order_products__train.csv") -->
<!-- ``` -->

<!-- ```{r, message = FALSE} -->
<!-- # check data -->
<!-- # kable(aisles) -->
<!-- # kable(departments) -->
<!-- # kable(head(products)) -->
<!-- ``` -->

<!-- #### Get a sample of 10000 'most recent' orders -->

<!-- ```{r, message = FALSE} -->
<!-- # Set seed for reproducibility -->
<!-- set.seed(1234) -->
<!-- orders_train_sample = orders_train[sample(nrow(orders_train), 10000), ] -->
<!-- #write.csv(orders_sample, "./input/orders_train_sample.csv", row.names = F) -->
<!-- ``` -->

<!-- #### Get a subset of prior records for user_ids in orders_sample -->

<!-- ```{r, message = FALSE} -->
<!-- # Merge products columns into order_products_prior -->
<!-- tmp <- merge(order_products_prior, products, by.x="product_id", by.y="product_id") -->
<!-- #tmp <- subset(tmp, select = -c(product_name)) -->
<!-- kable(head(tmp, 12)) -->

<!-- # Merge Orders columns into order_products_prior -->
<!-- orders_products_prior_merged <- merge(tmp, orders_prior, by.x="order_id", by.y="order_id") -->
<!-- kable(head(orders_products_prior_merged, 12)) -->
<!-- #write.csv(tmp, "./input/orders_products_prior_merged.csv", row.names = F) -->
<!-- ``` -->

<!-- #### Filter order_product_prior based on user_ids in orders_train_sample -->

<!-- ```{r, message = FALSE} -->
<!-- order_products_prior_sample = subset(orders_products_prior_merged, user_id %in% orders_train_sample$user_id) -->
<!-- #write.csv(order_products_prior_sample, "./input/order_products_prior_sample.csv", row.names = F) -->
<!-- ``` -->

<!-- #### Get a subset of train records for user_ids in orders_sample -->

<!-- ```{r, message = FALSE} -->
<!-- # Merge products columns into order_products_train -->
<!-- tmp <- merge(order_products_train, products, by.x="product_id", by.y="product_id") -->
<!-- #tmp <- subset(tmp, select = -c(product_name)) -->
<!-- kable(head(tmp, 12)) -->

<!-- # Merge Orders columns into order_products_train -->
<!-- orders_products_train_merged <- merge(tmp, orders_train, by.x="order_id", by.y="order_id") -->
<!-- kable(head(orders_products_train_merged, 12)) -->
<!-- #write.csv(tmp, "./input/orders_products_prior_merged.csv", row.names = F) -->
<!-- ``` -->

<!-- #### Filter order_product_train based on user_ids in orders_sample -->

<!-- ```{r, message = FALSE} -->
<!-- order_products_train_sample = subset(orders_products_train_merged, user_id %in% orders_train_sample$user_id) -->
<!-- #write.csv(order_products_train_sample, "./input/order_products_train_sample.csv", row.names = F) -->
<!-- ``` -->

<!-- #### Join order_products_prior_sample and order_products_train_sample into a single dataset -->
<!-- ```{r, message = FALSE} -->
<!-- order_products_sample_combined <- rbind(order_products_prior_sample, order_products_train_sample) -->
<!-- kable(head(order_products_sample_combined, 12)) -->
<!-- mydata <- subset(order_products_sample_combined, select = c(user_id, order_id, product_name)) -->
<!-- kable(head(mydata, 12)) -->

<!-- # # Convert eval_set column to number -->
<!-- # eval_set_no <- ifelse(order_products_sample_combined$eval_set=="train", 1, 0) -->
<!-- # order_products_sample_combined <- subset(order_products_sample_combined, select = -c(eval_set)) -->
<!-- # order_products_sample_combined <- data.frame(cbind(order_products_sample_combined,eval_set_no)) -->
<!-- # kable(head(order_products_sample_combined, 12)) -->

<!-- # Save file -->
<!-- write.csv(order_products_sample_combined, "./input/order_products_sample_combined.csv", row.names = F) -->
<!-- write.csv(mydata, "./input/mydata.csv", row.names = F) -->
<!-- ``` -->

```{r, message = FALSE}
order_products_sample_combined <- read.csv("./input/order_products_sample_combined.csv")
mydata <- read.csv("./input/mydata.csv")
```

#### Take a look at data distribution by Dept. id. 

```{r, message = FALSE, fig.width=8, fig.align='left'}
a=table(order_products_sample_combined$department_id)
barplot(a,main="Using BarPlot to display Dept.Id Comparision",
        ylab="Count",
        xlab="department_id",
        col=rainbow(5),
        #legend=rownames(a)
        )
```

#### Take a look at data distribution by Day of Week. 

```{r, message = FALSE, fig.width=8, fig.align='left'}
density1 <- seq(5,35,length.out=7)
a=table(order_products_sample_combined$order_dow)
wth <- c(rep(5, each=7))
barplot(a,main="Using BarPlot to display Day of Week Comparision",
        ylab="Count",
        xlab="department_id",
        col=rainbow(2, start = 0.5, end = 0.7),
        density=density1,
        #legend=rownames(a),
        width = wth)
```

#### Take a look at data distribution by Hour of Day. 

```{r, message = FALSE, fig.width=8, fig.height=5, fig.align='left'}
a=table(order_products_sample_combined$order_hour_of_day)
barplot(a,main="Using BarPlot to display Hour of Day Comparision",
        ylab="Count",
        xlab="department_id",
        col=rainbow(22, start = 0.5, end = 0.7),
        #legend=rownames(a),
        #width = rep(2, each=24)
        )
```

## 3. Data Preparation

#### a) Data Modification

#### 

```{r, message = FALSE}

```

#### Create Transaction Table 

```{r, message = FALSE}
# Create Transaction Table
retail <- mydata[complete.cases(mydata),]
glimpse(retail)
# str(retail)
# retail$Time <- as.factor(retail$order_hour_of_day)
# a <- hms(as.character(retail$order_hour_of_day))
# retail$Time = hour(a)
# retail %>%
#   ggplot(aes(x=Time)) +
#   geom_histogram(stat="count",fill="indianred")
# 
# 
# detach("package:plyr", unload=TRUE)
# 
# tmp <- retail %>%
#   group_by(product_id, product_name) %>%
#   summarize(count = n()) %>%
#   arrange(desc(count))
# tmp <- head(tmp, n=10)
# tmp
# tmp %>%
#   ggplot(aes(x=reorder(product_name,count), y=count))+
#   geom_bar(stat="identity",fill="indian red")+
#   coord_flip()

retail_sorted <- retail[order(retail$user_id),]
library(plyr)
itemList <- ddply(retail,c("user_id","order_id"),
                  function(df1)paste(df1$product_name,
                                     collapse = ","))


# Remove user and order id's from the list
itemList$user_id <- NULL
itemList$order_id <- NULL
colnames(itemList) <- c("Items")
write.csv(itemList,"InstaCart_MBA.csv", quote = FALSE, row.names = TRUE)
```

#### b) Feature Engineering

- 

## 4. Data Modeling

- Ethical Framework Questions: 
  - Does your use case require a more interpretable algorithm? Accuracy is more impotant than interpretability for this study.
  - Should you be optimizing for a different outcome than accuracy to make your outcomes fairer? Since there is no demographic data, fairness would be hard to determine.
  - Is it possible that a malicious actor has compromised training data and created misleading results? No. The data is from a reputable source.

## 4. A) Data Modeling - 

```{r, message = FALSE}
suppressWarnings(
tr <- read.transactions('InstaCart_MBA.csv', format = 'basket', sep=',')
)
```

```{r, message = FALSE}
tr
summary(tr)

itemFrequencyPlot(tr, topN=20, type='absolute', col=rainbow(20, start = 0, end = 0.6))
# Training Apriori on the dataset
rules <- apriori(tr, parameter = list(supp=0.001, conf=0.5))
rules <- sort(rules, by='confidence', decreasing = TRUE)
summary(rules)
inspect(rules)#[1:10])
```


## 5. A) Data Evaluation 

#### Test the Model

```{r, message = FALSE}
topRules <- rules[1:20]
# Visualising the results
#plot(topRules)
plot(topRules, method="graph", shading=NA)
plot(topRules, method = "grouped")
plot(topRules, method = "two-key plot")
plot(topRules, method = "scatterplot")
plot(topRules, method = "paracoord")
# plot(topRules, measure=c("support","lift"),shading="confidence",interactive=T)
```

#### Perform a test prediction

```{r, message = FALSE}
grocery_item = "Garlic"
rules <- apriori(tr, parameter = list(supp=0.001, conf=0.15), 
                 appearance = list(default="rhs", lhs=grocery_item), 
                 control = list (verbose=F))
rules_conf <- sort (rules, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
result = inspect(head(rules_conf))
result$rhs
```

## 6. Final Model Analysis and Selection

#### Cost Analysis


```{r, message = FALSE}

```

#### Model Comparison

- The following Machine Learning Algorithms were used in this analysis:
  - Apriori


```{r, message = FALSE}

```

#### Selected Model: 

## 7. Deployment

#### Shiny App Url: 

#### Summary Explanation

- Limitations of our analysis: 
  - 
  - 
  
- Further steps we could take: 
  - 
  - 
  
- Explanation of Model:
  - 
  - The Factors that contributed to malignant vs benign tumor identification were the following columns: 

- Ethical Framework Questions: 
  - Can a malicious actor infer information about individuals from your system? No. There is no PII present.
  - Are you able to identify anomalous activity on your system that might indicate a security breach? This would need to be considered for each specific deployment.
  - Do you have a plan to monitor for poor performance on individuals or subgroups? N/A since No demographic data is present.
  - Do you have a plan to log and store historical predictions if a consumer requests access in the future? N/A since No demographic data is present.
  - Have you documented model retraining cycles and can you confirm that a subject’s data has been removed from models? N/A since No demographic data is present.

## References

Yihui Xie, J. J. Allaire, Garrett Grolemund, 2019, R Markdown: The Definitive Guide
https://bookdown.org/yihui/rmarkdown/markdown-syntax.html

Jonathan McPherson, 2016, R Notebooks
https://blog.rstudio.com/2016/10/05/r-notebooks

Adam Kardash, Patricia Kosseim, 2018, Responsible AI in Consumer Enterprise, integrate.ai


---
title: "CSML1000 Winter 2020, Group 8, Assignment2: Market Basket Prediction Model"
author: "Steven Wang, Tarun Bagga, Paul Doucet, Jerry Khidaroo, Nicola Stevanovic"
date: "2/15/2020"
output:
   html_document:
   toc: TRUE
   toc_depth: 2
#output:
#  pdf_document:
#    toc: TRUE
#    toc_depth: 3
#    fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{css, echo=FALSE}
.ethical-text {
  background-color: lightblue;
  border: 3px solid blue;
  font-weight: italic;
}
```

## Load Libraries

```{r, message = FALSE}
# Load packages
# library('ggplot2') # visualization
# library('ggthemes') # visualization
# # library('scales') # visualization
# library('dplyr') # data manipulation
# library('caret')
# library('caret')
# library('readr')
# library('data.table')
# library('knitr')
# library('purrr') # ANN modeling functionality
# library('arules')

library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(lubridate)
library(arules)
library(arulesViz)
library(plyr)
```

## 1. Business Understanding

- Business Problem: The Business problem we are solving is how to increase the number of items ordered by a customer in each order submitted in the online store. To acheive this goal we want to build a machine learning model that predicts the most likely items a customer will add to their basket next based on their current selection and display those items as suggestions for quick access.

- Project Plan: 
  - Select instacart dataset from kaggle to solve this unsupervised learning problem.
  - Load and get an understanding of the dataset, its target variable and its features.
  - Make any modifications to the dataset needed to enable learning algorithms to be run on the data.
  - Identify the features of the dataset that are important in predicting the output recomendations.
  - Build and evaluate 2 Models by appling unsupervised machine learning algoritms to the dataset  as appropiate and testing them.
  - Identify the best model to use for the project.
  - Build a shiny app that deploys the selected model with a user interface that allows end users to add products to a basket and get a list of recomended products.
  - Identify any ethical considerations that should be addressed at each stage of the process.

- Business Success Criteria: 

- Ethical Framework Questions: 
  - How could your system negatively impact individuals? The greatest negative impact would occur for a false positive diagnosis since this could delay treatment and in a life threatening scenario. A false negative would also be negatively impactful but to a lessor degree.
  - Who is most vulnerable and why? The most vulnerable would be patients with a 'M' dianosis not detectable by the model.
  - How much error in predictions can your business accept for this use case? False positives need to be minimised as much as posible. Minimsing False negatives are second in priority.
  - Will you need to explain which input factors had the greatest influence on outputs? Yes. Being able to explain which features have the most influence on outcome is very desirable.
  - Do you need PII or can you provide group-level data? The analysis requires patient level data however any PII can be anonymised

## 2. Data Understanding

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>

<div class = "blue">

- Ethical Framework Questions: 
  - Have you de-identified your data and taken measures to reduce the probability of reidentification? The data is de-identified.
  - Will socially sensitive features like gender or ethnic background influence outputs? No demographic data is present.
  - Are seemingly harmless features like location hiding proxies for socially sensitive features? No demographic data is present. 
</div>

#### 2.1 Get Data Files

- For this assignment we came across classic kaggle problem from instacart for predicting next item in the basket using market basket modelling. We looked at the data. The Dataset used is obtained from: https://www.kaggle.com/c/instacart-market-basket-analysis

#### 2.2 Initial Data Collection Report:

  - There are seven files provided as part of the dataset for this unsupervised learning model:
    1. aisles.csv: 
    2. departmenst.csv:
    3. order_products__prior.csv:
    4. order_products__traing.csv:
    5. orders.csv
    6. products.csv
    7. sample_submission.csv (This file will not be used in the analysis since it is only relevant for submission to the kaggle competition where the data was sourced.).
    
    Each of the datset contains different type of data related to the online grocery mart with PK/FK relationship.
    
    Lets explore  and deepdive with the various datasets provided.
    
    Products
    
    FOr this section there are 2 CSV files provided, namely order_products_train and order_products_prior. These data inside the files specifies which products were purchased in each order. That is, order_products_prior contains previous order products for all customers and order_products_train contains the latest order products for some customers only.
    
Let us look the counts of records inside the files. Using the built-in R functions we can see that there are 1,384,617 products in the order_products_train file and 32,434,489 products in the order_products_prior file.
Both the CSV files contain 4 features:
    The ID of the order (order_id)
    The ID of the product (product_id)
    The ordering of that product in the order (add_to_cart_order)
    Whether that product was reordered (reordered).
    Overall, there are 3,346,083 unique orders for 49,685 unique products.
    
    Orders
Upon perusung the data for Instacart orders. The records inside the orders.csv file present a different tale as we see that there are 3,421,083 orders and 7 feature columns:
The ID of the order (order_id)
The ID of the customer (user_id)
Which evaluation datasets that the order is in — prior, train, or test (eval_set)
The number of the order (order_number)
The day of the week when that order occurred (order_dow)
The hour of the day when that order occurred (order_hour_of_day)
The number of days since the previous order (days_since_prior_order)
    
    Departments and Aisles
Let’s look at the most important departments, sorted by the number of products. The top 5 departments are Personal Care (6,563), Snacks (6,264), Pantry (5,371), Beverages (4,365), and Frozen (4,007).

Let’s look at the most important aisles over all departments, as being sorted by the number of products. Ignoring the ‘missing’ values, we have the top 5 aisles being Candy Chocolate (1,258), Ice Cream (1,091), Vitamins Supplements (1,038), Yogurt (1,026), and Chips Pretzels (989).
    
#### 2.3 Load and check data

```{r, message = FALSE}
orders = read.csv("./input/orders.csv")
aisles = read.csv("./input/aisles.csv")
departments = read.csv("./input/departments.csv")
products = read.csv("./input/products.csv")
order_products_prior = read.csv("./input/order_products__prior.csv")
order_products_train = read.csv("./input/order_products__train.csv")
```

```{r, message = FALSE}
# check data
kable(head(aisles))
kable(departments)
kable(head(products))
kable(head(orders))
kable(head(order_products_prior))
kable(head(order_products_train))
```

## 3. Data Preparation

###### We also know from the data file descriptions from the source website that order_products_train has the most recent procuct orders by the user and order_products_prior contains all of the historical product orders.

#### 3.1 Data Modification

###### Due to data storage and processing constraints this analysis will use a random sample of the dataset available rather than all records.

###### Get a sample of 10000 'most recent' orders

```{r, message = FALSE}
# Set seed for reproducibility
set.seed(1234)
orders_train <- subset(orders, eval_set=='train')
orders_train_sample = orders_train[sample(nrow(orders_train), 10000), ]
#write.csv(orders_sample, "./input/orders_train_sample.csv", row.names = F)
```

###### merge order_products_prior with products and orders tables

```{r, message = FALSE}
# Merge products columns into order_products_prior
tmp <- merge(order_products_prior, products, by.x="product_id", by.y="product_id")

# Merge Orders columns into order_products_prior
orders_prior <- subset(orders, eval_set=='prior')
orders_products_prior_merged <- merge(tmp, orders_prior, by.x="order_id", by.y="order_id")
```

###### Filter orders_products_prior_merged based on user_ids in orders_train_sample

```{r, message = FALSE}
order_products_prior_sample = subset(orders_products_prior_merged, user_id %in% orders_train_sample$user_id)
```

###### merge order_products_train with products and orders tables

```{r, message = FALSE}
# Merge products columns into order_products_train
tmp <- merge(order_products_train, products, by.x="product_id", by.y="product_id")

# Merge Orders columns into order_products_train
orders_products_train_merged <- merge(tmp, orders_train, by.x="order_id", by.y="order_id")
```

###### Filter order_product_train based on user_ids in orders_sample

```{r, message = FALSE}
order_products_train_sample = subset(orders_products_train_merged, user_id %in% orders_train_sample$user_id)
```

###### Since we will be performing an unsupervised machine learning analysis on the dataset, records from order_products_prior_sample and order_products_train_sample can be combined.

###### Join order_products_prior_sample and order_products_train_sample into a single dataset


```{r, message = FALSE}
order_products_sample_combined <- rbind(order_products_prior_sample, order_products_train_sample)

# Lets look at the merged sample dataset we will proceed with in our analysis
kable(head(order_products_sample_combined, 12))
```
#### 3.2 Feature Engineering

###### - For the Market Basket analysis we will need 3 data columns in order to build a transaction file:

###### - A Customer identity field, an Transaction field, and a Product Name field

###### - From looking at the features we have orders$user_id for Customer identity, orders$order_id for Transaction, and products$product_name for Product Name.

```{r, message = FALSE}
# Select the 3 columns we need to create the transactions dataset and inspect
mydata <- subset(order_products_sample_combined, select = c(user_id, order_id, product_name))
kable(head(mydata, 12))
```

###### These 2 files will now be saved to disk for future re-loading during project development

```{r, message = FALSE}
# Save files
write.csv(order_products_sample_combined, "./input/order_products_sample_combined.csv", row.names = F)
write.csv(mydata, "./input/mydata.csv", row.names = F)
```

###### Create Transaction Table 

```{r, message = FALSE}
# Create Transaction Table
detach("package:plyr", unload=TRUE)

retail_sorted <- mydata[complete.cases(mydata),]
retail_sorted <- retail_sorted[order(retail_sorted$user_id),]
library(plyr)
itemList <- ddply(retail_sorted,c("user_id","order_id"),
                  function(df1)paste(df1$product_name,
                                     collapse = ","))

# Remove user and order id's from the list
itemList$user_id <- NULL
itemList$order_id <- NULL
colnames(itemList) <- c("Items")
```

```{r, message = FALSE}
# Save Transaction File
write.csv(itemList,"./input/InstaCart_MBA.csv", quote = FALSE, row.names = TRUE)
```

- Clear up some memory

```{r, message = FALSE}
rm(orders)
rm(order_products_prior)
rm(order_products_train)
rm(orders_products_train_merged)
rm(orders_train)
rm(orders_train_sample)
rm(order_products_prior_sample)
rm(order_products_train_sample)
rm(orders_products_prior_merged)
rm(orders_prior)
rm(order_products_sample_combined)
rm(mydata)
gc()
```
###### Once the analysis data files are saved the data preparation steps above can be commented out and we can just:

###### Load the saved files

```{r, message = FALSE}
order_products_sample_combined <- read.csv("./input/order_products_sample_combined.csv")
mydata <- read.csv("./input/mydata.csv")
```

## 4. Data Understanding (Continued)

###### Back to some more detailed examination of the data now that we have our analysis dataset in place

###### Take a look at data distribution by Dept. id. 

```{r, message = FALSE, fig.width=8, fig.align='left'}
a=table(order_products_sample_combined$department_id)
barplot(a,main="Using BarPlot to display Dept.Id Comparision",
        ylab="Count",
        xlab="department_id",
        col=rainbow(5),
        #legend=rownames(a)
        )
```

###### Take a look at data distribution by Day of Week. 

```{r, message = FALSE, fig.width=8, fig.align='left'}
density1 <- seq(5,35,length.out=7)
a=table(order_products_sample_combined$order_dow)
wth <- c(rep(5, each=7))
barplot(a,main="Using BarPlot to display Day of Week Comparision",
        ylab="Count",
        xlab="department_id",
        col=rainbow(2, start = 0.5, end = 0.7),
        density=density1,
        #legend=rownames(a),
        width = wth)
```

###### Take a look at data distribution by Hour of Day. 

```{r, message = FALSE, fig.width=8, fig.height=5, fig.align='left'}
a=table(order_products_sample_combined$order_hour_of_day)
barplot(a,main="Using BarPlot to display Hour of Day Comparision",
        ylab="Count",
        xlab="department_id",
        col=rainbow(22, start = 0.5, end = 0.7),
        #legend=rownames(a),
        #width = rep(2, each=24)
        )
```

```{r, message = FALSE}
# tmp <- retail %>%
#   group_by(product_id, product_name) %>%
#   summarize(count = n()) %>%
#   arrange(desc(count))
# tmp <- head(tmp, n=10)
# tmp
# tmp %>%
#   ggplot(aes(x=reorder(product_name,count), y=count))+
#   geom_bar(stat="identity",fill="indian red")+
#   coord_flip()
```

## 5. Data Modeling

- Ethical Framework Questions: 
  - Does your use case require a more interpretable algorithm? Accuracy is more impotant than interpretability for this study.
  - Should you be optimizing for a different outcome than accuracy to make your outcomes fairer? Since there is no demographic data, fairness would be hard to determine.
  - Is it possible that a malicious actor has compromised training data and created misleading results? No. The data is from a reputable source.

#### 5.1 Data Modeling - Apriori

```{r, message = FALSE}
# Load Transaction data
suppressWarnings(
tr <- read.transactions('./input/InstaCart_MBA.csv', format = 'basket', sep=',')
)
```

###### 5.1.1 Build Model - Apriori

```{r, message = FALSE}
tr
summary(tr)

itemFrequencyPlot(tr, topN=20, type='absolute', col=rainbow(20, start = 0, end = 0.6))
# Training Apriori on the dataset
rules <- apriori(tr, parameter = list(supp=0.001, conf=0.5))
rules <- sort(rules, by='confidence', decreasing = TRUE)
summary(rules)
inspect(rules[1:20])
```

#### 5.2 Data Evaluation - Apriori

###### 5.2.1 Test the Model

```{r, message = FALSE, fig.align='left'}
topRules <- rules[1:20]
# Visualising the results
plot(topRules, method="graph", shading=NA)
```

```{r, message = FALSE, fig.height=8}
plot(topRules, method = "grouped")
```

```{r, message = FALSE}
plot(topRules, method = "two-key plot")
plot(topRules, method = "scatterplot")
plot(topRules, method = "paracoord")
# plot(topRules, measure=c("support","lift"),shading="confidence",interactive=T)
#interestMeasure(rules.pruned, c("support", "chiSquare", "confidence", "conviction", "cosine", "coverage", "leverage", "lift", "oddsRatio"), titanic.raw)
```


```{r, message = FALSE}

```

###### 5.2.2 Perform a test prediction

```{r, message = FALSE}
grocery_item = "Milk"
rules <- apriori(tr, parameter = list(supp=0.001, conf=0.15), 
                 appearance = list(default="rhs", lhs=grocery_item), 
                 control = list (verbose=F))
rules_conf <- sort (rules, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
result = inspect(head(rules_conf))
result$rhs
```

#### 5.3 Data Modeling - Eclat

```{r, message = FALSE}
# Training Eclat on the dataset
eclat_itemsets = eclat(tr, parameter = list(support = 0.001, minlen = 2))
eclat_itemsets <- sort(eclat_itemsets, by='count', decreasing = TRUE)
summary(eclat_itemsets)

inspect(sort(eclat_itemsets, by = 'count')[1:10])
eclat_topItemSets <- eclat_itemsets[1:10]
```

#### 5.4 Data Evaluation - Eclat

```{r, message = FALSE, fig.align='left'}
# Visualising the results of the Eclat Analysis
plot(eclat_topItemSets, method="graph")
plot(eclat_topItemSets, method = "scatterplot")
plot(eclat_topItemSets, method = "paracoord")
```

###### 5.4.1 Perform a test prediction

```{r, message = FALSE}
grocery_item = "Garlic"
# eclat_itemsets = eclat(tr, parameter = list(support = 0.001, minlen = 2), 
#                  control = list (verbose=F))

## Create rules from the itemsets
eclat_rules <- ruleInduction(eclat_itemsets, tr, confidence = .9)
summary(eclat_rules)
kable(inspect(eclat_rules[1:20]))

result_eclat <- subset(eclat_rules, subset = lhs %in% "Milk")

rules_conf <- sort (result_eclat, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
result = inspect(head(rules_conf))
result$rhs
```

## 6. Final Model Analysis and Selection

#### 6.1 Cost Analysis

#### 6.2 Model Comparison

- The following Machine Learning Algorithms were used in this analysis:
  - Apriori
  - Eclat
  
Using associative rule methodology in apriori, the model provides us next product sequence the customer purchases. Since it is difficult to understand nuances of teh model. the results can be interpreted using Lift, support and confidence.
We define the association rules as follow:
Support – It is defined as the percentage of transactions that comprise all of the items in a dataset. The more the support value the more frequently the product occurs. High support values are preferred for ample amount of future
transactions.

Confidence - It is the probability that a transaction that contains the items on the left hand side of the also
contains the item on the right hand side. The more the confidence value, the greater the likelihood that the
product on the right hand side will be purchased.

Lift - Lift is nothing but the ratio of Confidence to Expected Confidence. It is the probability of all of the products
in a rule occurring together by the product of the probabilities of the items on the left and right hand side
occurring as if there was no association between them.

#Discuss the interpretation here
Some of the association rules which were interpreted from our methodology are mentioned below:
##Please paste the associate rules table here

```{r, message = FALSE}

```

#### 6.3 Selected Model: 
Based on the associative rule methodology and models to predict the reorder of products, some of the
recommendations have been made:
• It will be productive to run promotional and marketing campaigns with the help of the associative rules.
Based on the prediction of the next product, customers can be given additional offers by bundling the
products together for a lesser price and customize the products based on the association rules
• Based on the reordering model, personalized communications can be very lucrative by reminding the
customers to reorder the products or can be added to the cart automatically based on the customer
preferences
• We would recommend Instacart to add the products directly to the customer’s cart or to provide a
suggestion list when they make their purchase in order to enhance the customer experience
• By knowing the rate of products reordered, Instacart can make use of the reordered data to analyze the
inventory stocks by ensuring the replenishments and proper scheduling of the products to increase internal
productivity.

## 7. Deployment

#### 7.1 Shiny App Url: 

#### 7.2 Summary Explanation

- Limitations of our analysis: 
  - Due to processing and resource limitations we used a random sample of aproximately 10% of the original dataset
  - The analysis is based on data provided by InstaCart and may inherit any biases that exists in their customer base relative to the general population.
  
- Further steps: 
  - The analysis can be expanded to include all of the original data as well as any other similar sources that may be available.
  - 
  
- Explanation of Model:
  - The model 
  - The following Factors contributed to the success of the model in predicting user ordering behaviour: 

- Ethical Framework Questions: 
  - Can a malicious actor infer information about individuals from your system? No. There is no PII present.
  - Are you able to identify anomalous activity on your system that might indicate a security breach? This would need to be considered for each specific deployment.
  - Do you have a plan to monitor for poor performance on individuals or subgroups? N/A since No demographic data is present.
  - Do you have a plan to log and store historical predictions if a consumer requests access in the future? N/A since No demographic data is present.
  - Have you documented model retraining cycles and can you confirm that a subject’s data has been removed from models? N/A since No demographic data is present.

## References

Yihui Xie, J. J. Allaire, Garrett Grolemund, 2019, R Markdown: The Definitive Guide
https://bookdown.org/yihui/rmarkdown/markdown-syntax.html

Jonathan McPherson, 2016, R Notebooks
https://blog.rstudio.com/2016/10/05/r-notebooks

Adam Kardash, Patricia Kosseim, 2018, Responsible AI in Consumer Enterprise, integrate.ai

